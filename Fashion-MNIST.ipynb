{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MNIST Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most commonly used datasets for educational purposes is the MNIST dataset. You can download it from deep learning libraries such as keras and pytorch and it consists of 28x28 pixel greyscale images of handwritten digits. Each image has the corresponding label and it can be used to train and test neural network models for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NLZathBN9php"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillemguigoicorominas/opt/anaconda3/envs/workspace/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use PyTorch to build out deep learning models. The following cells are for you to get familiar with how pytorch tensors work.\n",
    "\n",
    "*The following cells are just for you to execute and observe the result. Jump to the section* **Working with Fashion-MNIST** *for the explanation of the code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FnAdkv7iO1X0"
   },
   "outputs": [],
   "source": [
    "# pytorch and numpy work well together\n",
    "some_numpy_array = np.array([1,2,3])\n",
    "some_torch_tensor = torch.tensor(some_numpy_array)\n",
    "some_numpy_array = some_torch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddBnXjLvAdkH",
    "outputId": "24d61d3a-f4f0-48da-c674-76e685e25bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8YVOSRCMqj8",
    "outputId": "f4bb60e0-fb80-4dca-fcba-398a4e211e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1385, -1.9960, -0.1867,  1.0266,  0.2241,  0.1065, -0.2873,\n",
      "          -1.0284, -0.2260, -1.7151, -0.7120,  0.9874, -0.1214,  0.3057,\n",
      "          -1.1367,  0.2897]]])\n",
      "torch.Size([1, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "# define tensor of random numbers\n",
    "input_data = torch.randn(1,1,16)\n",
    "print(input_data)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTd-KHAOM4qS",
    "outputId": "24c2579a-8ac0-4ff1-ff13-0a6f10a01462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.1552,  0.1579, -0.2182,  0.1385,  0.1622,  0.1536,  0.0953, -0.2122,\n",
       "                       -0.0872,  0.2143, -0.0605,  0.0430, -0.1235, -0.1568,  0.0833, -0.0841],\n",
       "                      [-0.2183,  0.0568, -0.0457,  0.0840,  0.0922,  0.0995,  0.0674, -0.0522,\n",
       "                        0.1183, -0.0008,  0.1452, -0.1566, -0.0540,  0.2314,  0.0280, -0.0215],\n",
       "                      [-0.0823,  0.0081, -0.2248, -0.0915,  0.2265,  0.1924,  0.1825, -0.1071,\n",
       "                        0.2453,  0.0882,  0.2127, -0.1657,  0.2261,  0.0047, -0.0637, -0.1187],\n",
       "                      [ 0.1275, -0.0618,  0.0055, -0.2158,  0.0968, -0.1722, -0.1163, -0.1412,\n",
       "                        0.0352, -0.1097,  0.1018,  0.2007,  0.0968,  0.2381,  0.2307,  0.1870],\n",
       "                      [ 0.1983,  0.1759,  0.0005, -0.0186, -0.0642,  0.0044,  0.0999,  0.2150,\n",
       "                       -0.2245, -0.1428,  0.1788, -0.1638,  0.0723, -0.1883,  0.0256, -0.2121],\n",
       "                      [-0.2051,  0.0867, -0.0375, -0.2421, -0.0446, -0.0164,  0.2030, -0.0699,\n",
       "                        0.0465,  0.0489, -0.0463, -0.0268,  0.2054, -0.1193,  0.1017,  0.0814],\n",
       "                      [ 0.2445, -0.0609,  0.1881,  0.2031,  0.0484, -0.2196,  0.2048,  0.1432,\n",
       "                        0.0210,  0.0701, -0.0278, -0.1885,  0.2087, -0.0195, -0.1476, -0.1207],\n",
       "                      [-0.1396, -0.1535,  0.1943, -0.1708,  0.2473, -0.0082, -0.0251,  0.0139,\n",
       "                        0.1258, -0.1565, -0.0480,  0.0482, -0.0619,  0.0234,  0.2334,  0.0879]])),\n",
       "             ('bias',\n",
       "              tensor([-0.2315,  0.2109,  0.0877, -0.1658, -0.0356, -0.0333,  0.2357, -0.1517]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define linear layer\n",
    "linear = nn.Linear(16, 8)\n",
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAfm8mkyBFtl",
    "outputId": "ace54e3e-485c-4d6a-9376-a84f4405ebbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.state_dict()['bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08rpwCTRBgp0",
    "outputId": "51016b95-d28b-4788-b823-787a3b9d7e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=16, out_features=8, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNTzlnLuBjRE",
    "outputId": "b7c41384-a223-4ea3-dba1-bea1283cc24c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slgJLHZiOOyL",
    "outputId": "c33c3e6f-b304-4f95-b83c-32180e74b796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "output = linear(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvQc8Z7rB4dT",
    "outputId": "0efbe844-db22-44f7-9c47-26fc9b7f75bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npITLguRB3Ab",
    "outputId": "52bc4c40-90e7-4c1e-a6c6-90121d250285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5559, -0.0164, -0.3728,  0.0934, -0.7925, -0.7157,  0.1560,\n",
       "           0.0678]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySThG-I1OsWR",
    "outputId": "1d9104ad-4e7b-4ed1-ff9c-f5a5972b87ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5559, -0.0164, -0.3728,  0.0934, -0.7925, -0.7157,  0.1560,\n",
       "           0.0678]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the same output using matrix multiplication (torch.matmul(t1, t2))\n",
    "torch.matmul(input_data, linear.state_dict()['weight'].T) + linear.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Working with Fashion-MNIST</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is the \"hello, world!\" of datasets for image classification. Since it is arguably overused, today we will download the Fashion-MNIST dataset. As explained by the authors:\n",
    "\n",
    "\"Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\"\n",
    "\n",
    "You can find the original repository [here](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jOs6qq-_Mkm7"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST('./fmnist_data', download=True, train=True, transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                ]))\n",
    "\n",
    "test_dataset = datasets.FashionMNIST('./fmnist_data', download=True, train=False, transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you execute this code PyTorch will download the dataset and store it in the specified folder (./fmnist_data). You can load it afterwards at any time without having to download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./fmnist_data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./fmnist_data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital images consist of pixels that (typically) range from 0 to 255 in value, consisting of an 8-bit representation of color ($2^8=256$ values). Most digital images are RGB, which combine different values for Red, Green and Blue pixels between 0 and 255. You can imagine this as three images superimposed on top of eachother that combined make the colors of the visible spectrum. These are called *channels*, and a typical RGB image has 3. The images in the current dataset have a single channel, as they are grayscale. The pixel values in the raw images range from 0 to 255, but here we already normalized the data with mean 0.1307 and standard deviation 0.3081 in the loading step. These are arbitrary values taken from PyTorch's own tutorial on how to work with the Fashion-MNIST dataset (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Each element in the train and test datasets is a tuple that contains both the image in tensor form and the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(type(image))\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of [1, 28, 28] specifies that the image has a height and width of 28 pixels and a single channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch's tutorial page we can find the following labels map, which tells us the classes that correspond to each numeric label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first image in the training set is an Ankle Boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.02, 'Ankle Boot')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(labels_map.get(label), fontsize=14, y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data in the train and test datasets tells us that there are 60.000 images in the train set and 10.000 images in the test set.\n",
    "\n",
    "We will use PyTorch's DataLoader to iterate over the samples in the training and test sets. Here is where we will select the batch size, i.e. the number of samples that we will use as input at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3QuJIOzePM8s"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "VgPtwoVVQVSL"
   },
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.linear_1 = nn.Linear(784, 64)\n",
    "        self.linear_2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.linear_1(x))\n",
    "        x = F.sigmoid(self.linear_2(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "iDDq_Q4ERh_9"
   },
   "outputs": [],
   "source": [
    "# here we specify that we want to use the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = FullyConnected().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jlmYAqqETXUc"
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "4H8Li9ZfRAYo",
    "outputId": "b25e1619-8cbd-4f95-a5f1-515e2f07e58b"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [512, 10], got [512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workspace/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workspace/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workspace/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [512, 10], got [512]"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_epoch_train, loss_epoch_test = 0, 0\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        input_data = img.to(device).reshape(batch_size, 1, -1)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(input_data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss values for this epoch\n",
    "        loss_epoch_train += loss.item()\n",
    "\n",
    "        for im, lab in zip(outputs, labels):\n",
    "            if im.argmax() == lab.argmax():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy_train_epoch = correct / total\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, (img, labels) in enumerate(test_loader):\n",
    "        input_data = img.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_data)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_epoch_test += loss.item()\n",
    "        for im, lab in zip(outputs, labels):\n",
    "            if im.argmax() == lab.argmax():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        accuracy_test_epoch = correct / total\n",
    "\n",
    "    print(f'epoch: {epoch + 1}, training loss: {loss_epoch_train / (i+1)}, test loss: {loss_epoch_test / (j+1)}')\n",
    "    print(f'epoch: {epoch + 1}, training accuracy: {accuracy_train_epoch}, test accuracy: {accuracy_test_epoch} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmtTPZd8Hsip",
    "outputId": "7f05f66d-da00-4069-cd2d-b81153593a93"
   },
   "outputs": [],
   "source": [
    "example = 25\n",
    "model(testset[example][0].to(device).reshape(1,-1)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQrggxymPDXb",
    "outputId": "f933384b-3950-45f0-c043-a866858b8f23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[example][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "M7oHw8YlPPCp",
    "outputId": "4f65f8b3-0a0c-4d85-e1d0-4ab99f514ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7e8ad8282e30>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_dataset[example][0].numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRxUBGKyPgpJ"
   },
   "source": [
    "<h3>Exercise</h3>\n",
    "Repeat all the steps above using the original MNIST dataset. It is up to you to decide the architecture of the network and hyperparameters (learning rate, number of epochs and batch size). Can you get over 95% accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
